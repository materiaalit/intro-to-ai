---
  title: Part 2
  exercise_page: true
  quiz_page: false
  published: true
---


<% partial 'partials/exercise', locals: { name: 'TravelPlanner v2.0 (AStar) (2p)' } do %>

<p>
  This week's first exercise is a continuation of the TravelPlanner
  from last week. The only difference is the change of algorithm:
  instead of breadth-first search, you should now implement A*
  search.
</p>

<p>
  The cost associated with each transition from one stop to the next
  is the time required for the transition. This includes both the time
  spent waiting for the tram and the actual travel time. In our imaginary
  scenario, all trams leave at regular 10 minute intervals from their
  first stop (each line is given both ways, so another tram will leave
  at the same time from the other end).
</p>

<p>
  Your route query will be given in the form:
<pre>
      TravelPlanner.search(departureStop, destinationStop, departureTime)
</pre>
  where the departure time can be given in minutes since the last full
  10 minutes since the timetable of our tram network is so regular.
</p>

<p>
  You'll need to complete the following steps (these instructions apply
  directly to Java but the general idea is the same in other
  languages):
  <ol>
    <li> Implement a <code>StateComparator</code> class (the Java package
      already has a template for this) with
      method <code>heuristic(Stop s)</code>, which calculates a
      lower bound on the time required to reach the destination
      from stop <code>s</code>.  A lower bound can be obtained by
      computing the distance between the two stops, and dividing it by
      the maximum speed of the tram which you can assume to be 260
      coordinate points per minute.
    <li> Also implement method <code>compare(State s1, State s2)</code>
      in class <code>StateComparator</code> so that it can be used to
      order the nodes in the priotity queue based on <code>cost
	+ h(node)</code> as described in Part 1. Here the nodes are
      states that are defined by the stop and the time since
      departure.
    <li> Implement the A* search in class <code>TravelPlanner</code>.
      The <code>PriorityQueue</code> data structure comes in handy. An
      instance of the <code>StateComparator</code> class should be given as
      an argument to the <code>PriorityQueue</code> constructor.
  </ol>
</p>

<p>
  As before, you should return the obtained route as a backward-linked
  list of states.</p>

<p>
  The <code>Stop</code> class in the Java package provides
  functionality for listing the neighboring stops and for getting the
  minimum time required to get there (including the waiting time).
</p>

<p>
  If you prefer to not use the Java package and TMC, that's totally
  fine.  You'll find the stop information in <code>network.json</code>
  and the route descriptions in <code>lines.json</code>, both of which
  are self-explanatory.
</p>

<% end %>


You can find the lecture slides for this part on the <a href="https://courses.helsinki.fi/en/data15001/124845454">course homepage</a>
under Materials.

<% partial 'partials/material_heading' do %>
  Games
<% end %>

<p>
  This week, we will study a classic AI problem: games.
  The simplest scenario, which we will focus on for the sake of
  clarity, are two-player, perfect-information games such as
  tic-tac-toe and chess.
</p>

<p>
  Another topic that we'll be able to get started with, and continue
  on next week, is reasoning under uncertainty using probability.
</p>

<% partial 'partials/hint', locals: { name: 'Learning objectives of Part 2' } do %>

<table class="table">
  <tr>
    <th>
      Theme
    </th>
    <th>
      Objectives (after the course, you ...)
    </th>
  </tr>
  <tr>
    <td>
      Games and search (continued from last week)
    </td>
    <td>
      <ul>
	<li>can formulate a simple game (such as tic-tac-toe) as a game tree
	<li>can explain and implement the minimax algorithm and depth-limited alpha-beta pruning
	<li>can design a reasonable heuristic evaluation function in a game (e.g., chess)
      </ul>
    </td>
  </tr>
  <tr>
    <td>
      Reasoning under uncertainty (to be continued next week)
    </td>
    <td>
      <ul>
	<li>can express uncertain knowledge in a simple situation using a probabilistic model
	<li>can apply the Bayes theorem to calculate posterior probabilities given evidence in a simple scenario
      </ul>
    </td>
  </tr>
</table>

<% end %>

<% partial 'partials/material_heading' do %>
  Games
<% end %>

<p>
  Maxine and Minnie are true game enthusiasts. They just love games.
  Especially two-person, perfect information games such as tic-tac-toe
  or chess.
</p>

<p>
  One day they were playing tic-tac-toe. Maxine, or Max as her
  friends call her, was playing with X. Minnie, or Min as her
  friends call her, had the Os. The situation was
<pre>
       O| |O
       -+-+-
       X| |
       -+-+-
       X|O|
</pre>
  Max was looking at the board and contemplating her next move, as it
  was her turn, when she suddenly buried her face in her hands in
  despair, looking quite like Garry Kasparov playing Deep Blue in
  1997.
</p>

<p>
  Yes, Min was close to getting three Os on the top row, but Max could
  easily put a stop to that plan. So why was Max so pessimistic?
</p>

<% partial 'partials/material_sub_heading' do %>
  Game Trees
<% end %>

<p>
  To analyse games and optimal strategies, we will introduce the
  concept of a <b>game tree</b>. The game tree is similar to a search
  tree, such as the one in the Sudoku example discussed at the
  lecture.  (Remember that you should also study the lecture slides in
  addition to this material. Some material may be discussed in one but
  not the other.) The different states of the game are represented by
  nodes in the game tree. The "children" of each node N are the
  possible states that can be achieved from the state corresponding to
  N. In board games, the state of the game is defined by the board
  position and whose turn it is.
</p>

<p>
  Consider, for example, the following game tree which begins
  not at the root but in the middle of the game (because otherwise,
  the tree would be way too big to display).
  <br>
  <img width=80% src="/img/diagrams/tictactoe-tree.png">
  <br>
  The game continues at the board position shown in the root node,
  numbered as (1) at the top, with Min's turn to place O at any of the
  three vacant cells. Nodes (2)--(4) show the board positions
  resulting from each of the three choices respectively. In the next
  step, each node has two possible choices for Max to play X each,
  and so the tree branches again.
</p>

<p>
  The game ends when either player gets a row of three, or when there
  are no more vacant cells. When starting from the above starting
  position, the game always ends in a row of three.
</p>

<p>
  Now consider nodes (5)--(10) on the second layer from the bottom.
  In nodes (7) and (9), the game is over, and Max wins with three X's
  in a row. In the remaining nodes, (5), (6), (8), and (10), the game
  is also practically over, since Min only needs to place her O
  in the only remaining cell to win. We can thus decide that the
  end result, or the <b>value</b> of the game in each of the nodes
  on the second level from the bottom is determined. For the
  nodes that end in Max's victory, we'll say that the value
  equals +1, and for the nodes that end in Min's victory, we'll say
  that the value is -1.
</p>

<p>
  More interestingly, let's now consider the next level of nodes
  towards the root, nodes (2)--(4). Since we decided that both of the
  children of (2), i.e., nodes (5) and (6), lead to Min's victory, we
  can without hesitation attach the value -1 to node (2) as well.  For
  node (3), the left child (7) leads to Max's victory, +1, but the
  right child (8) leads to Min winning, -1. However, it is Max's turn
  to play, and she will of course choose the left child without
  hesitation. Thus, every time we reach the state in node (3), Max
  wins. Thus we can attach the value +1 to node (3).
</p>

<p>
  The same holds for node (4): again, since Max can choose where to
  put her X, she can always ensure victory, and we attach the value
  +1 to node (4).
</p>

<p>
  So far, we have decided that the value of node (2) is -1, which
  means that if we end up in such a board position, Min can ensure
  winning, and that the reverse holds for nodes (3) and (4): their
  value is +1, which means that Max can be sure to win if she only
  plays her own turn wisely.
</p>

<p>
  Finally, we can deduce that since Min is an experienced player, she
  can reach the same conclusion, and thus she only has one real
  option: give Max an impish grin and play the O in the middle of the
  board.
</p>

<p>
  In the diagram below, we have included the value of each node as
  well as the optimal game play starting at Min's turn in the root
  node.
  <br>
  <img width=80% src="/img/diagrams/tictactoe-values.png">
</p>

<p>
  The value of the root node, which is said to be the <b>value of the
  game</b>, tells us who wins (and how much, if the outcome is not
  just plain win or lose): Max wins if the value of the game is +1,
  Min if the value is -1, and if the value is 0, then the game will
  end in a draw. This all is based on the assumption that both players
  choose what is best for them.
</p>

<p>
  The optimal play can also be deduced from the values of the nodes:
  at any <b>Min node</b>, i.e., node where it is Min's turn, the
  optimal choices are given by those children whose value is minimal,
  and conversely, at any <b>Max node</b>, where it is Max's turn, the
  optimal choices are given the the children whose value is maximal.
</p>

<% partial 'partials/material_sub_heading' do %>
  Minimax Algorithm
<% end %>

<p>
  We can exploit the above concept of the value of the game to obtain
  an algorithm with optimal game play in, theoretically speaking, any
  deterministic, two-person, perfect-information game. Given a state
  of the game, the algorithm simply computes the values of the
  children of the given state and chooses the one that has the maximum
  value if it is Max's turn, and the one that has the minimum value if
  it is Min's turn.
</p>

<p>
  The algorithm can be implemented using the neat recursive
  functions below for Max and Min nodes respectively. This is
  known as the <b>Minimax algorithm</b> (see <a href="https://en.wikipedia.org/wiki/Minimax">Wikipedia: Minimax</a>).
</p>

<% partial 'partials/code_highlight' do %>
    max_value(node):
1:     if end_state(node): return value(node)
2:     v = -Inf
3:     for each child in node.children():
4:        v = max(v, min_value(child))
5:     return v
<% end %>

<% partial 'partials/code_highlight' do %>
    min_value(node):
1:     if end_state(node): return value(node)
2:     v = +Inf
3:     for each child in node.children():
4:        v = min(v, max_value(child))
5:     return v
<% end %>

<% partial 'partials/exercise', locals: { name: 'Why so serious, Max? (1p)' } do %>

<p>
  Let's return to the tic-tac-toe game described in the beginning
  of this section. To narrow down the space of possible end-games
  to consider, we can observe that Max must clearly place an X on
  the top row to avoid imminent defeat:
  <pre>
       O|X|O
       -+-+-
       X| |
       -+-+-
       X|O|
  </pre>
  Now it's Min's turn to play an O. Evaluate the value of this state
  of the game as well as the other states in the game tree where the
  above position is the root, using the Minimax algorithm.
</p>

<% end %>


<% partial 'partials/hint', locals: { name: 'Sounds good, can I go home now?' } do %>

<p>
  As stated above, the Minimax algorithm can be used to implement
  optimal game play in any deterministic, two-player,
  perfect-information game. Such games include tic-tac-toe, connect
  four, chess, Go, etc. (Rock-paper-scissors is not in this class of
  games since it involves information hidden from the other player;
  nor are Monopoly or backgammon which are not deterministic.) So as
  far as this topic is concerned, is that all folks, can we go home
  now?
</p>

<p>
  The answer is that in theory, yes, but in practice, no. In many
  games, the game tree is simply way too big to traverse in full.  For
  example, in chess the average branching factor, i.e., the average
  number of children (available moves) per node is about 35.  That
  means that to explore all the possible scenarios up to only two
  moves ahead, we need to visit approximately 35 x 35 = 1225 nodes
  -- probably not your favorite pencil-and-paper homework exercise...  A
  look-ahead of three moves requires visiting 42875 nodes; four moves
  1500625; and ten moves 2758547353515625 (that's about
  2.7 quadrillion) nodes.
</p>

<p>
  In Go, the average branching factor is estimated to be about 250.
  Go means no-go for Minimax.
</p>

<% end %>

<p>
  Next, we will learn a few more tricks that help us manage massive
  game trees, and that were crucial elements in IBM's Deep Blue
  computer defeating the chess world champion, Garry Kasparov, in
  1997.
</p>


<% partial 'partials/material_sub_heading' do %>
  Depth-limited minimax and heuristic evalution criteria
<% end %>

<p>
  If we can afford to explore only a small part of the game tree,
  we need a way to stop the minimax recursion before reaching an
  end-node, i.e., a node where the game is over and the winner
  is known. This is achieved by using a <b>heuristic
    evaluation function</b> that takes as input a board position,
  including the information about which player's turn is next,
  and returns a score that should be an estimate of the likely
  outcome of the game continuing from the given board position.
</p>

<p>
  Good heuristics for chess, for example, typically count the amount
  of material (pieces) weighted by their type: the queen is usually
  considered worth about two times as much as a rook, three times a
  knight or a bishop, and nine times as much as a pawn. The king is of
  course worth more than all other things combined since losing it
  amounts to losing the game. Further, occupying the strategically
  important positions, e.g., near the middle of the board, is
  considered an advantage.
</p>

<p>
  The minimax algorithm presented above requires minimal changes
  to obtain a <b>depth-limited</b> version where the heuristic
  is returned at all nodes at a given depth limit.
</p>

<% partial 'partials/material_sub_heading' do %>
  Alpha-beta pruning
<% end %>

<p>
  Another breakthrough in game AI, proposed independently by several
  researchers including John McCarthy in and around 1960,
  is <a href="https://en.wikipedia.org/wiki/Alpha%E2%80%93beta_pruning">alpha-beta
  pruning</a>. For small game trees, it can be used independently of
  the heuristic evaluation method, and for large trees, the two can be
  combined into a powerful method that has dominated the area of game
  AI for decades.
</p>

<p>
  A good example of the idea behind alpha-beta-pruning can be seen
  in the tic-tac-toe game tree that we discussed above -- scroll
  up and let the image of the root node burn into your retina.
</p>

<p>
  Now simulate the Minimax algorithm at the stage where the value of
  the left child node, -1, has been computed and returned to the
  <code>min_value</code> function. The next step would be to
  call <code>max_value</code> to compute the value of the middle
  child. But hold on! If the left child guarantees victory for Minnie,
  what does it matter how the game ends if she chooses to play any
  other way? As soon as the algorithm finds a child node with the best
  possible outcome for the player whose turn it is, it can make a
  choice and avoid computing the values of all the other child nodes.
</p>

<p>
  To implement this in a similar fashion as the Minimax algorithm
  requires small changes in the <code>min_value</code> and
  <code>max_value</code> functions. Understanding the connection
  between these changes and the principle illustrated by the above
  pruning example is not as easy as it may sound, so please pay
  close attention to this topic and work out the examples and
  exercises with care.
</p>

<% partial 'partials/code_highlight' do %>
    max_value(node, alpha, beta):
1:     if end_state(node): return value(node)
2:     v = -Inf
3:     for each child in node.children():
4:        v = max(v, min_value(child, alpha, beta))
5:        alpha = max(alpha, v)
6:        if alpha >= beta: return v
7:     return v
<% end %>

<% partial 'partials/code_highlight' do %>
    min_value(node, alpha, beta):
1:     if end_state(node): return value(node)
2:     v = +Inf
3:     for each child in node.children():
4:        v = min(v, max_value(child, alpha, beta))
5:        beta = min(beta, v)
6:        if alpha >= beta: return v
7:     return v
<% end %>

<p>
  An important thing to remember is that the <code>alpha</code> value
  is updated only at the Max nodes, and the <code>beta</code> value is
  updated only at the Min nodes. The updated values are passed as
  arguments down to the children, but not up to the calling parent
  node. (That is, the arguments are passed <i>as values</i>, not <i>as
  references</i> in programming lingo.)
</p>

<p>
  The interpretation <code>alpha</code> and <code>beta</code> is that
  they provide the interval of possible values of the game at the node
  that is being processed:
  <code>alpha</code> &le; value &le; <code>beta</code>. This interval
  is updated during the algorithm, and if at some point, the interval
  shrinks so that <code>alpha = beta</code>, we know the value and
  can return it to the parent node without processing any more
  child nodes. It can also happen that <code>alpha > beta</code>,
  which implies that the current node will never be visited in
  optimal game play, and its processing can likewise be aborted.
</p>

<p>
  When starting the recursion at the root node, we use the minimum and
  maximum value of the game as the <code>alpha</code>
  and <code>beta</code> values respectively. For tic-tac-toe and
  chess, for instance, where the outcome is plain win/loss, this
  is <code>alpha = -1</code> and <code>beta = 1</code>. If the range
  of possible values is not specified in advance, we initialize
  as <code>alpha = -&infin;</code> and <code>beta = &infin;</code>.
</p>

<p>
  It is useful to work out a few examples to really understand the
  beauty of alpha-beta pruning. Here's another tic-tac-toe
  example.
  <br>
  <img width=80% src="/img/diagrams/tictactoe-alphabeta.png">
  <br>
</p>

<p>
  You should simulate the algorithm to see that the two branches
  that are grayed out  indeed get pruned -- therefore, it is
  actually a bit misleading to even show their minimax values
  since the algorithm never computes them.
</p>

<p>
  Remember that the Max nodes (such as the root node) only update
  the <code>alpha</code> value and pass it down to the next
  child node. Check that you reach a situation where
  <code>alpha=0</code> and  <code>beta=-1</code> in a
  Min node. Actually you should reach such a situation twice.
</p>

<p>
  Another good example (except for the choice of colors) can be found
  from Bruce Rosen's lecture notes for <i>Fundamentals of Artificial
  Intelligence - CS161</i> at
  UCLA: <a href="http://web.cs.ucla.edu/~rosen/161/notes/alphabeta.html">here</a>.
  Note in particular how he emphasizes the fact that the tree is only
  expanded node by node as the algorithm runs, instead of running the
  algorithm on a full tree as is often suggested by diagrams such as
  the ones we use in this material (shame on us!).  Rosen's example
  also illustrates a scenario where the value of the game is not
  constrained to be –1,0, or +1, and the algorithm starts at the root
  with <code>alpha = –&infin;</code> and
  <code>beta = &infin;</code>.
</p>

<% partial 'partials/exercise', locals: { name: 'Alpha-beta for tic-tac-toe (1p)' } do %>

<p>
  Maxine wants to try again -- best out of three! Minnie agrees and
  after a while, they arrive at the following position
  <pre>
       O|X|O
       -+-+-
       X| |X
       -+-+-
        |O|
  </pre>
  It's again Min's turn to play an O. Evaluate the value of this state
  of the game as well as the other states in the game tree where the
  above position is the root, using alpha-beta pruning. Start the
  recursion by calling <code>min-value(root, -1, 1)</code> where
  <code>root</code> is the above board position.
</p>

<p>
  What is the value of the game, and what are the optimal moves?
</p>

<% end %>


<p>
  Notice that while the left-to-right order of the children of each
  node, i.e., the order in which the children are processed in the
  loop on lines 4--7, is arbitrary, it can have significant impact
  on which branches are pruned.
</p>

<% partial 'partials/exercise', locals: { name: 'Alpha-beta programming exercise (1p)' } do %>

<p>
  Now we feel confident enough about concepts and ideas behind the
  Minimax algorithm and alpha-beta pruning that we can start
  programming. We'll implement a tic-tac-toe bot.
</p>

<p>
  <ol>
    <li> First implement the basic framework for the game: a
      data structure (class) for board positions, the functions for
      checking whether either player has a row of three, and a
      function for producing the list of children for a given board
      position.
    <li> Then implement the Minimax algorithm according to the pseudo-code
      given above. The algorithm should return for any given board
      position, the value of the game: -1, 0, or 1.
    <li> Using the previous step, modify the program so that it also
      outputs the optimal move for the player whose turn it is (which
      is given as an input, of course). You can do this by keeping
      record of the child node that yields the highest (lowest) value
      in a Max (Min) node.
    <li> You can test the solution at this stage by using it to solve
      the above pencil-and-paper exercises.
    <li> As the culmination of this exercise, modify the algorithm to
      implement alpha-beta pruning as detailed in the pseudo-code
      above. To see whether you gain any speed-up, you can see how
      long the algorithm runs (or how many recursive function calls
      are performed) with or without alpha-beta pruning -- simply
      comment out line 7 with the test <code>alpha >= beta</code> to
      revert back to vanilla Minimax.
  </ol>

  As a bonus question, you can think about how to order the child
  nodes so that as much pruning is done as possible.

<% end %>


<% partial 'partials/material_heading' do %>
  Reasoning under Uncertainty
<% end %>

<p>
  In this section, we'll study methods that can cope with
  uncertainty. The history of AI has seen various competing approaches
  to handling uncertain and imprecise information: fuzzy logic,
  non-monotonic logic, the theory of plausible reasoning <i> et
  cetera</i>. In most cases, probability theory has turned out to be
  the most principled and consistent paradigm for reasoning under
  uncertainty.
</p>

<p>
  If you want an objective and diplomatic view on the matter, don't
  ask me. I'll just repeat a counter-argument made by a proponent of
  fuzzy logic when he tried to get around a rigorous formal proof
  showing that any method of inference where degrees of plausibility
  are represented as real numbers, other than probability theory, is
  inconsistent: the <a href="http://sipi.usc.edu/~kosko/Fuzziness_Vs_Probability.pdf">response of the fuzzy logic proponent</a> was that
  since the proof is based on <i>ordinary (two-valued) logic, it
    doesn't apply to fuzzy logic.</i> Now that is a counter-argument
  that is hard to refute!
</p>

<% partial 'partials/material_sub_heading' do %>
  Probability Fundamentals
<% end %>

<p>
  We are working to add material about the basic concepts of
  probability theory: probabilistic model, events, joint
  probability, conditional probability, etc.
</p>

<p>
  Since this section is not finished yet, feel free to refresh these
  concepts from your favorite source.  Two great examples are:
  <ul>
    <li>Ian Goodfellow, Yoshua Bengio & Aaron Courville: <a href="http://www.deeplearningbook.org/">Deep Learning</a> (Section 3)
    <li>Charles Grinstead & J. Laurie Snell: <a href="http://www.dartmouth.edu/~chance/teaching_aids/books_articles/probability_book/amsbook.mac.pdf">Introduction to Probability</a> (pdf file, 3.2MB, 520 pages)
  </ul>
  The Bayes rule (aka Bayes theorem) is the single most important tool
  from probability calculus, and we'll be using in again and again for
  building AI applications, here is some more material about it
  in particular:
  <ul>
    <li><a href="http://en.wikipedia.org/wiki/Bayes%27_theorem#Drug_testing">Wikipedia: Bayes theorem (Section <i>Drug testing</i>)</a>
    <li><a href="http://www.youtube.com/watch?v=tRE6mKAIkno">Youtube: Maths Doctor, A Level Statistics 1.5 Bayes' Theorem and medical testing</a>
    <li>Better Explained: <a href="http://betterexplained.com/articles/understanding-bayes-theorem-with-ratios/">Understanding Bayes Theorem with Ratios</a>
  </ul>
</p>



<% partial 'partials/exercise', locals: { name: 'Rolling the Dice (1p)' } do %>

<p>
  The lecturer has two dice in his office (true story). One of them is
  an ordinary die and will yield any result from one to six with equal
  probability.  However, the other die is loaded (true story, again).
  It will yield a six with probability 1/2 and any other outcome with
  probability 1/10. He will roll the ordinary die first, and then
  the loaded one. Calculate the following probabilities. Recall
  that <i>P(A|B)</i> means the conditional probability of <i>A</i>
  given <i>B</i>.
</p>

<p>
  <ol>
    <li> P("both outcomes are 6")
    <li> P("neither outcome is 6")
    <li> P("the sum of the outcomes equals 9")
    <li> P("the sum of the outcomes equals 9" | "at least one outcome is 6")
    <li> P("at least one outcome is 6" | "the sum of the outcomes equals 9")
  </ol>
</p>

<p>
  <i>Hint:</i> We know that probability calculus can be confusing, but
  keep calm and carry on, you'll get the hang of it!
</p>

<% end %>


<% partial 'partials/exercise', locals: { name: 'Bayes rule (1p)' } do %>

<p>
  Let's use the same dice as in the previous exercise.
</p>

<p>
  <ol>
    <li> What is the distribution of the sum of the outcomes when
      rolling the ordinary die twice?
    <li> What is the distribution of the sum of the outcomes when
      rolling the loaded die twice?
    <li> Assume that either die (ordinary or loaded) is selected
      at random with probability 1/2 each. The selected die is
      rolled twice. Calculate the probability<br><br>
      <i>P("the die is loaded" | "the sum of the outcomes is 10")</i><br><br>
      Calculate the probability that the die is ordinary as well to
      make sure that the sum of the two probabilities equals one.
    <li> Again, either die is selected with equal probability.
      We notice that every time the die is rolled, the outcome is six.
      After how many rolls can we be at least 99% sure that the
      die is loaded? <br><br> <i>Hint:</i> Calculate the ratio of the
      posterior probabilities of each die after <i>k</i>
      rolls. If the posterior probability of the loaded die is over
      99%, then the ratio of the posterior probabilities is over
      0.99/0.01 = 99.
  </ol>
</p>

<p>
  If you have trouble with these exercises, you may wish to study
  additional material on the Bayes rule. Good pointers are provided above.
</p>

<% end %>
