<!-- Exercise 1.1 -->

<% partial 'partials/solution', locals: { name: 'Example solution' } do %>

<p>
  Most of the articles tend to be quite technical and focused
  on a narrow sub-problem.
</p>

<p>
  The technical background required often includes maths such as
  formal logic and probability calculus, and computer science
  topics such as algorithms and data structures.
</p>

<p>
  To take a concrete example, we can take a look at the article
  <a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/viewFile/15001/14466">Polynomial Optimization Methods for Matrix Factorization</a>
  by P.-W. Wang, C.-L. Li, and J.Z. Kolter, which appeared in the
  AAAI-17 conference.
</p>

<p>
  <ol>
    <li>
      The article presents new algorithms for factorizing matrices
      (representing big matrices as products of smaller matrices)
      that are based on certain optimization techniques. Matrix
      factorization can be used in various machine learning scenarios.
      For example, many state-of-the-art recommender systems that
      predict user ratings of items such as movies or music include
      matrix factorization as a key component. Improving matrix
      factorization algorithms will thus eventually lead to better
      movie and music recommendations.
    <li>
      The article is clearly relevant to the course topics
      (machine learning and recommendation systems in particular).
    <li>
      Based on this article alone, AI research would appear to
      differ only slightly from mathematics research. The emphasis
      on practical solutions and empirical evaluation is a feature
      characteristic to AI and ML research.
    <li>
      The paper uses advanced multivariate calculus tools. From an
      algorithmic point of view, there is actually nothing intricate
      (basically a few loops), and the maths background is the area
      where further studies would be required to get the details.
    <li>
      The article doesn't mention consciousness or any other
      "philosophical" issues at all. The problem is clearly
      constrained as that of minimizing a function. The progress
      in AI that this paper makes is quantitative, not
      qualitative.
  </ol>
</p>

<% end %>

<!-- Exercise 1.2 -->

<% partial 'partials/solution', locals: { name: 'Example solution' } do %>

<p>
  Here's the traversal order in the format <code>node: [node list]</code>:
<pre>
      BFS            DFS
      a: [b]         a: [b]
      b: [c,f]       b: [f,c]
      c: [f,e,i]     f: [g,d,c]
      f: [e,i,d,g]   g: [h,d,c]
      e: [i,d,g]     h = goal
      i: [d,g]
      d: [g]
      g: [h]
      h = goal
</pre>
</p>

<% end %>

<!-- Exercise 1.3 -->

<% partial 'partials/solution', locals: { name: 'Example solution' } do %>

<p>
  <ol>
    <li>
      Here's the state diagram (from <a href="https://en.wikipedia.org/wiki/Tower_of_Hanoi">Wikipedia: Towers of Hanoi</a>):<br><br>
      <img src="/img/diagrams/880px-Tower_of_Hanoi-3.svg.png" width=62%><br><br>
      The encoding is such that the first letter in a three-letter
      sequenceencodes the position of the smallest disk
      (<code>a</code> = left, <code>b</code> = middle,
      <code>c</code> = right), the second letter encodes the
      position of the middle disk, and the third letter encodes the
      position of the largest disk. Thus, there is a transition from the
      starting state <code>aaa</code> to states <code>baa</code> and
      <code>caa</code>, where the smallest disk is move to the middle
      or the right-most peg respectively.
    <li>
      BFS visits the states in the following order:
      <code>aaa [start], baa, caa, bca, cba, aca, cca, aba, bba,
	ccb, bbc, acb, bcb, abc, cbc, abb, bab, acc, cac,
	bbb, cbb, aab, cab, bcc, ccc [goal]</code>.
      Minor differences are possible due to different ordering of
      the neighbors of each node.
    <li>
      DFS visits the states in the following order:
      <code>aaa [start], caa, cba, bba, bbc, cbc, cac, bac, bcc, ccc
	[goal]</code>.
      Here the order in which the neighbors are considered can have a
      significant effect on the result. In the worst case, all other
      nodes are visited before expanding the goal node.
    <li>
      <i>a)</i> The path produced by BFS is the one with the least
      possible transitions, seven. The path produced by DFS, on the
      other hand, is the same as the sequence of states visited by
      DFS, which is ten transitions in length.
      <i>b)</i> BFS visited 25 states altogether while DFS only visited
      11 states. So BFS visits much more states but produces a shorter
      path.
    <li>
      The number of states visited by BFS varies only slightly, and
      the path that it produces is always the same (since the shortest
      path is unique). However, DFS can either go straight to the
      goal by visiting only the states that are on the shortest path,
      or it can visit almost all states in the state space, and
      produce the maximally long path from the start to the goal.
  </ol>
</p>

<p>
  On the bonus exercise, see the Wikipedia page mentioned above.
</p>

<% end %>

<!-- Exercise 1.4 -->

<% partial 'partials/solution', locals: { name: 'Example solution' } do %>

<p>
  You can download a working solution <a href="https://materiaalit.github.io/intro-to-ai-18/files/TravelPlanner.java">here</a>.
</p>

<% end %>

<!-- Exercise 2.1 -->

<% partial 'partials/solution', locals: { name: 'Example solution' } do %>

<p>
  An example Java solution is <a href="https://materiaalit.github.io/intro-to-ai-18/files/astar-solution.zip">here</a>.
</p>

<% end %>

<!-- Exercise 2.2 -->

<% partial 'partials/solution', locals: { name: 'Example solution' } do %>

<p>
  Here's the game tree with the values of each node.
</p>

<img src="/img/exercises/ex2/tree_Ex2_2.png" width=80%>

<p>
  As you can see, Max has all the reason to be serious since by
  playing in the bottom-right corner (Node (4)), Min can guarantee a
  win (just follow the orange arrows). The inevitable victory of Min
  can also be seen from the value of the game at the root node which
  is &ndash;1.
</p>

<% end %>

<!-- Exercise 2.3 -->

<% partial 'partials/solution', locals: { name: 'Example solution' } do %>

<p>
  As was stated in the exercise, the recursion
  is started by calling
  <code>min-value</code> for the root node (Node (1) below) with
  arguments
  <code>alpha=-1, beta=1</code>. These values are then passed to the
  child nodes, starting with the leftmost child (Node (2)), and from
  it further down all the way to the leaf node (5). The return value,
  which is passed up the parent node (3), is &ndash;1 since in this
  case, Min wins with three Os in a row.  Strictly speaking, this
  already causes pruning to occur in Node (3): it is a Min node, and
  thus it updates its beta value, and the beta value becomes
  &ndash;1. The test <code>alpha =-1 &le; beta=-1</code> (line 7
  in the pseudocode) triggers the return condition. However, the
  effect is null since there wouldn't have been any other children of
  Node (3) anyway.
</p>

<img src="/img/exercises/ex2/alphabeta_Ex2_3.png" width=60%>

<p>
  Next, Node (3) returns the value &ndash;1 to its parent, Node (2).
  As a Max node, Node (2) would update its alpha value but since the
  value returned by the child is &ndash;1 which is already the same
  as the current alpha value, no update occurs (line 6 in the
  pseudocode). The processing of nodes (4) and (6) is analogous to
  that of nodes (3) and (5) respectively.
</p>

<p>
  Interesting things start to happen in the root node when the value
  &ndash;1 is returned from Node (2). The root node is a Min node and
  it updates its beta value to be <code>beta=-1</code> (line 6 in the
  pseudocode), and therefore, the test <code>alpha=-1 &le;
  beta=-1</code> again triggers pruning and the other branches of the
  tree need not be traversed at all!
</p>

<p>
  The optimal game play can now be obtained by always choosing the
  child node with the minimum (maximum) value in Min (Max)
  nodes <i>among the child nodes that were visited</i>. The result is
  guaranteed to be the same as that of the Minimax algorithm, i.e.,
  optimal play for both players. For Max this is bad news: it's Min's
  victory even best-out-of-three...
</p>

<% end %>

<!-- Exercise 2.4 -->

<% partial 'partials/solution', locals: { name: 'Example solution' } do %>

<p>
  An example solution is <a href="https://materiaalit.github.io/intro-to-ai-18/files/Alphabeta_model_solution.zip">here</a> (zip archive).
</p>

<% end %>

<!-- Exercise 2.5 -->

<% partial 'partials/solution', locals: { name: 'Example solution' } do %>

<p>
  <ol>
    <li> Because the outcomes are independent, we can use the product rule to
      obtain<br><br>
      P("both outcomes are 6") = P("first outcome is 6") &times; P("second outcome is 6")<br>
	&nbsp;&nbsp;&nbsp;= 1/6 &times; 1/2 = 1/12 &asymp; 0.083<br><br>
    <li>
      P("neither outcome is 6") = P("first is not 6") &times; P("second is not 6") = 5/6 &times; 1/2 = 5/12 &asymp; 0.417<br><br>
    <li>
      Let's denote the result of the two dice with the pair (m,n), where m is the
      outcome of the first (fair) die, and n is the outcome of the second (loaded) die.
      Together the outcomes define the elementary event. The sum equals nine in the
      following elementary events: (3,6), (4,5), (5,4), (6,3). Note that the results
      (3,6) and (6,3) are distinct elementary events.<br><br>

      The result is obtained by summing up the probabilities of the aforementioned
      elementary events:<br><br>
      P((3,6)) + P((4,5)) + P((5,4)) + P((6,3)) =
	1/6 &times; 1/2 + 1/6 &times; 1/10 + 1/6 &times; 1/10 + 1/6 &times; 1/10
	= 8/60 &asymp; 0.133.<br><br>
    <li>
      Denote by A the event "the sum of the outcomes equals 9", and by B the event
      "at least one outcome is 6". Since the conditional probability is given by
      P(A | B) = P(A, B) / P(B), we start by calculating P(A, B) and
      P(B).<br><br>

      The elementary events where both A and B occur are
      (3,6) and (6,3). Thus, P(A,B) = 1/6 &times; 1/2 + 1/6 &times; 1/10 =
	6/60. Event B, on the other hand, is the complement of event
      "neither outcome is 6" from item (b) above, so we can get its probability
      easiest by applying the rule P(B) = 1 &ndash; P(&not;B), to get
      P(B) = 1 &ndash; 5/12 = 7/12.<br><br>

      Thus we get the conditional probability by substituting into the formula
      P(A | B) = P(A, B) / P(B) = 6/60 / (7/12) = 6/35 &asymp; 0.171.<br><br>
    <li>
      With the notation of the previous item, we get
      P(B | A) = P(A, B) / P(A) = 6/60 / (8/60) = 6/8 &asymp; 0.75, where
      P(A) = 8/60 was calculated in item (c) above.
  </ol>
</p>

<% end %>

<!-- Exercise 2.6 -->

<% partial 'partials/solution', locals: { name: 'Example solution' } do %>

<p>
  <ol>
    <li>
      The "distribution of the outcomes" means a list of probabilities for
      each possible outcome (here: sums of the two dice). The sum can take
      any value between 2, ..., 12. With a fair die, each elementary event
      (m,n) has probability 1/6 &times; 1/6 = 1/36, so the distribution is
      obtained by counting how many elementary events correspond to each
      possible sum and multiplying this by 1/36.<br><br>

      [P("sum is 2"), ..., P("sum is 12")] = [1/36, 2/36, 3/36, 4/36, 5/36,
      6/36, 5/36, 4/36, 3/36, 2/46, 1/36]<br><br>

    <li>
      With the loaded die, the elementary events have different
      probabilities, so a simple counting and multiplication won't do.
      Every event (m,n) where m&lt;6 and n&lt;6 has probability 1/10 &times;
      1/10 = 1/100. Every event (m,n) where m&lt;6 and n=6, or m=6 and n&lt;6,
      has probability 1/10 &times; 1/2 = 1/20. And finally, the event
      (6,6) has probability 1/2 &times 1/2 = 1/4.<br><br>

      We now get, for example, the probability that the sum is 9 from
      <br><br>

      P((3,6)) + P((4,5)) + P((5,4)) + P((6,3)) = 1/20 + 1/100 + 1/100 + 1/20
      = 12/100 = 0.12<br><br>

      The complete distribution is given by<br><br>

      [P("sum is 2"), ..., P("sum is 12")]<br>
      = [1/100, 2/100, 3/100, 4/100, 5/100, 14/100, 13/100, 12/100, 11/100, 10/100, 25/100]<br><br>

      To make sure we haven't messed up (who, me?), we can sum up the
      probabilities to check that the total is 1.0; i.e.,
      1+2+3+4+5+14+13+12+11+10+25=100. Yes!
<br><br>

    <li>
      Here we can apply the Bayes formula:<br><br>
      <pre>
      P("loaded die" | "the sum is 10")
         P("the sum is 10" | "loaded die") &times P("loaded die")
       = ---------------------------------------------------
                          P("the sum is 10")</pre>

      The first term in the numerator was calculated in the previous item, and
      the other term is known to be 1/2 by assumption. However, we'll have to
      calculate the annoying denominator by splitting it into two alternative
      ways to get the sum 10.<br><br>

      P("sum is 10" | "fair die") &times; P("fair die") + P("sum is 10" | "loaded die")
      &times; P("loaded die") <br>
      = 3/36 &times; 1/2 + 11/100 &times 1/2 = 29/300 &asymp; 0.0966.<br><br>

      In case you have difficulty following these calculations, please
      take a look at the additional material on Bayes rule above.
      Note that the numerical values of both of the conditional
      probabilities in the above expression come from the two
      distributions we calculated in items 1 & 2.<br><br>

      Substituting these values into the Bayes rule we get
      <br><br>
      P("loaded die" | "sum is 10") = 11/100 &times 1/2 / (29/300) = 33/58 &asymp; 0.5689.
      <br><br>

      When calculating the conditional probability P("fair die" | "sum
      is 10") by the Bayes rule, we'll end up with the very same
      denominator, whose value we already calculated to be 29/300,
      and thus we get:
      <br><br>
      P("fair die" | "sum is 10") = 3/36 &times; 1/2 / (29/300) =
      25/58.<br><br>

      The sum of the probabilities of the loaded and the fair die is
      33/58 + 25/58 = 1  as it should!<br><br>

      <i>Note:</i> The end result of the above calculation is that
      observing that the sum is 10 has almost no effect on our belief about
      which die was chosen: before the observation, our prior probability
      for the loaded die was 0.5, and after the observation, our posterior
      probability is 0.5689. You can think about what outcome would have
      changed our belief the most.<br><br>

    <li>
      Following the hint, it'll be enough to check whether<br><br>
      <pre>
	  P("loaded die" | "k times 6")
          ----------------------------- > 99
           P("fair die" | "k times 6")
      </pre>
      The posterior probability ratio can be obtained by applying the
      Bayes rule to both of the conditional probabilities above:<br><br>
      <pre>
	  P("k times 6" | "loaded die") &times P("loaded die")
          -----------------------------------------------
                          P("k times 6")
        --------------------------------------------------
	    P("k times 6" | "fair die") &times P("fair die")
            -------------------------------------------
                          P("k times 6") </pre>
      where the annoying denominator cancels. Since the prior probabilities
      are the same (0.5), they too cancel (which doesn't happen in general).
      What is left is the ratio of the likelihood terms:<br><br>
      P("k times 6" | "loaded die") / P("k times 6" | "fair die")
      = (1/2)<sup>k</sup> / (1/6)<sup>k</sup> = 3<sup>k</sup>.<br><br>
      The smallest k for which 3<sup>k</sup> &gt; 99 is k=5. Thus, five
      rolls are required until the posterior probability of the loaded die
      is greater than 99%.
  </ol>
</p>

<% end %>

<!-- Exercise 3.2 --> 

<% partial 'partials/solution', locals: { name: 'Example solution' } do %>

<p>
  <ol>
    <li>
      Dividing the occurrence counts by the total number of words
      in each class gives
      <pre>
        word     spam      ham
 ----------- -------- --------
 million     0.001629 0.000320
 dollars     0.000303 0.000388
 adclick     0.000532 0.000001
 conferences 0.000001 0.000039</pre>
      where we used the lower bound 0.000001 in case of zero counts.<br><br>

    <li>
      First of all, we can use <code>P(Word &ne; 'million')
      = 1 - P(Word = 'million')</code>. The probability on the right
      needs to be computed by the same technique as we've applied
      to obtain the annoying denominator in Bayes rule calculations,
      i.e., dividing it into the two possible ways to get the word
      'million':
      <br><br>
      <pre>
   P(Word = 'million')
   = P(spam) P(Word = 'million' | spam)
   + P(ham) P(Word = 'million' | ham)
   &asymp; 0.5 &times; 0.001629 + 0.5 &times; 0.000320 &asymp; 0.000974</pre>
      Thus, we get <code>P(Word &ne; 'million') &asymp; 1-0.000974
    = 0.999346</code>.<br><br>
    <li>
      <pre>
   P(spam | 'million') = P(spam) P('million' | spam) / P('million')
   &asymp; 0.5 &times; 0.001629 / 0.000974 &asymp; 0.836
</pre>
      This is a straightforward application of the Bayes rule where
      the annoying denominator was known from the previous item.
      If you are still uncertain about this, please review the
      material on Bayes rule and the references given in Section 3.1
      of Part 2.<br><br>
    <li>
      In this case, it is most convenient to use the trick for
      avoiding the computation of the annoying denominator (see above).
      Hence, we'll calculate the ratio of posterior probabilities<br><br>
      <pre>
    P(spam | 'million', 'dollars', 'adclick', 'conferences')
R = --------------------------------------------------------
     P(ham | 'million', 'dollars', 'adclick', 'conferences')

    P(spam) P('million'|spam) P('dollars'|spam) P('adclick'|spam) P('conferences'|spam)
  = -----------------------------------------------------------------------------------
      P(ham) P('million'|ham) P('dollars'|ham) P('adclick'|ham) P('conferences'|ham)

    0.5 &times; 0.001629 &times; 0.000303 &times; 0.000532 &times; 0.000001
  = ----------------------------------------------- &asymp; 54.23
    0.5 &times; 0.000320 &times; 0.000388 &times; 0.000001 &times; 0.000039</pre>

      Note that 54.23 is clearly not the required <i>probability</i>. To get
      the probability from the ratio R, we use the formula
      Prob = R/(1+R):<br><br>
      <pre>
P(spam | 'million', 'dollars', 'adclick', 'conferences') &asymp; 54.23 / (1+54.23) &asymp; 0.982</pre>
  </ol>
</p>

<% end %>

<!-- Exercise 3.3 --> 

<% partial 'partials/solution', locals: { name: 'Example solution' } do %>

<p>
  An example solution is <a href="https://materiaalit.github.io/intro-to-ai-18/files/SpamHam-solution.zip">here</a> (zip archive).
</p>

<% end %>

<!-- Exercise 4.1 --> 

<% partial 'partials/solution', locals: { name: 'Example solution' } do %>

<p>
  <ol>
    <li>
      The number of combinations is k<sup>N</sup> = k<sup>4</sup>.
      Since the probabilities of the elementary events must sum to
      one, it is enough to give k<sup>4</sup>&ndash;1 probability
      values to define the model this way.<br><br>
    <li>
      In an empty network, we only need to specify the distributions
      of each variable. Since each variable has k possible values, we
      need k&ndash;1 probabilities per variable, and the total number
      is N(k&ndash;1) = 4(k&ndash;1).<br><br>
    <li>
      The more edges there are in the network, the more parameters are
      required in order to specify all the conditional
      distributions. Thus, we need to calculate how many parameters
      there are in the "full" network. A full network has one node
      with no parents, one node with one parent, one node with two
      parents, etc.<br><br>

      For a node with m parents, the number of possible parent
      configurations is k<sup>m</sup> (since we assumed that the
      number of values that each parent can take is k). To specify
      the conditional distribution for all parent configurations,
      we thus need k<sup>m</sup>(k&ndash;1) probability parameters.<br><br>

      Summing up the numbers over the N=4 nodes, we thus get k&ndash;1
      + k(k&ndash;1) + k<sup>2</sup>(k&ndash;1) +
      k<sup>3</sup>(k&ndash;1).
      <br><br>

    <li>
      For general N, the above formula becomes<br><br>
      (1 + k + k<sup>2</sup> + ... + k<sup>N&ndash;1</sup>)(k&ndash;1)
      = k<sup>N</sup>&ndash;1,
      <br><br>
      where we used the formula for the sum of a geometric series.
  </ol>
<% end %>

<!-- Exercise 4.4 -->

<% partial 'partials/solution', locals: { name: 'Example solution' } do %>

<p>
  An example solution is <a href="https://materiaalit.github.io/intro-to-ai-18/files/Perceptron-solution.zip">here</a>.
</p>

<% end %>

<!-- Exercise 4.5 -->

<% partial 'partials/solution', locals: { name: 'Example solution' } do %>

<p>
  An example solution is <a href="https://materiaalit.github.io/intro-to-ai-18/files/NearestNeighbor.java">here</a>.
</p>

<% end %>

<!-- Exercise 5.1 -->

<% partial 'partials/solution', locals: { name: 'Example solution' } do %>

<p>
  With the default settings, MLP achieves to modest error rate of
  47.06&nbsp;%. Increasing the number of training steps
  (<code>trainingTime</code>) from 500 to 5000 only makes things
  worse: 48.53&nbsp;%.
</p>

<p>
  If you set the number of hidden layers to be one
  (setting <code>hiddenLayers = 1</code>), the error rate becomes
  45.59&nbsp;%, and with two hidden layers (which was apparently
  the default setting) you get again 48.53&nbsp;%.
  Things start to improve only when you add one more hidden
  layer (<code>hiddenLayers = 3</code>), and you get 30.88&nbsp;%
  error, and finally, with four hidden layers, the error becomes
  as low as 5.88&nbsp;%.
</p>

<% end %>

<!-- Exercise 5.2 -->

<% partial 'partials/solution', locals: { name: 'Example solution' } do %>

<p>
  <ol>
    <li>
      The NN classifier works really well, and we an get 100&nbsp;%
      accuracy straight off the bat. Intuitively, the NN classifier
      seems like a good idea for this data where the training data
      is relatively dense and nearby instances have the same
      class value.<br><br>
    <li>
      With default settings, the decision tree classifier
      <code>Trees/J48</code> yields error rate 11.77&nbsp;%.<br><br>
    <li>
      Looking at the decision boundaries, the NN classifier appears
      to work because the distances between instances in different
      classes is big enough. The decision tree classifier produces
      decision boundaries that are composed of axis-aligned lines.
      This doesn't sound like a good idea for the spirals data.
      In this light, the classification accuracy (11.77&nbsp;%) is
      actually not bad.
  </ol>
</p>

<% end %>

<!-- Exercise 5.5 -->

<% partial 'partials/solution', locals: { name: 'Example solution' } do %>

<p>
  The CYK table is as follows:
</p>

<p>
  <table style="text-align: center; padding: 6px;">
    <tr>
      <td  style="background-color: #ecf0f1; border: 1px solid #7fb3d5;">
      S</td>
    </tr>
    <tr>
      <td  style="background-color: #ecf0f1; border: 1px solid #7fb3d5;">
      &nbsp;</td>
      <td  style="background-color: #ecf0f1; border: 1px solid #7fb3d5;">
      VP<sup>3</sup></td>
    </tr>
    <tr>
      <td  style="background-color: #ecf0f1; border: 1px solid #7fb3d5;">
      S</td>
      <td  style="background-color: #ecf0f1; border: 1px solid #7fb3d5;">
      </td>
      <td  style="background-color: #ecf0f1; border: 1px solid #7fb3d5;">
      NP<sup>2</sup></td>
    </tr>
    <tr>
      <td  style="background-color: #ecf0f1; border: 1px solid #7fb3d5;">
      &nbsp;</td>
      <td  style="background-color: #ecf0f1; border: 1px solid #7fb3d5;">
      VP<sup>2</sup></td>
      <td  style="background-color: #ecf0f1; border: 1px solid #7fb3d5;">
      </td>
      <td  style="background-color: #ecf0f1; border: 1px solid #7fb3d5;">
      PP</td>
    </tr>
    <tr>
      <td  style="background-color: #ecf0f1; border: 1px solid #7fb3d5;">
      S</td>
      <td  style="background-color: #ecf0f1; border: 1px solid #7fb3d5;">
      </td>
      <td  style="background-color: #ecf0f1; border: 1px solid #7fb3d5;">
      NP</td>
      <td  style="background-color: #ecf0f1; border: 1px solid #7fb3d5;">
      </td>
      <td  style="background-color: #ecf0f1; border: 1px solid #7fb3d5;">
      NP</td>
    </tr>
    <tr>
      <td  style="background-color: #ecf0f1; border: 1px solid #7fb3d5;">
	S
      </td>
      <td  style="background-color: #ecf0f1; border: 1px solid #7fb3d5;">
	VP
      </td>
      <td  style="background-color: #ecf0f1; border: 1px solid #7fb3d5;">
      </td>
      <td  style="background-color: #ecf0f1; border: 1px solid #7fb3d5;">
	PP
      </td>
      <td  style="background-color: #ecf0f1; border: 1px solid #7fb3d5;">
      </td>
      <td  style="background-color: #ecf0f1; border: 1px solid #7fb3d5;">
	PP
      </td>
    </tr>
    <tr>
      <td  style="background-color: #ecf0f1; border: 1px solid #7fb3d5; width: 120px;">
	N,NP<br>
	<i>Google</i>
      </td>
      <td  style="background-color: #ecf0f1; border: 1px solid #7fb3d5; width: 120px;">
	V,VP<br>
	<i>bought</i>
      </td>
      <td  style="background-color: #ecf0f1; border: 1px solid #7fb3d5; width: 120px;">
	N,NP<br>
	<i>DeepMind</i>
      </td>
      <td  style="background-color: #ecf0f1; border: 1px solid #7fb3d5; width: 120px;">
	P<br>
	<i>for</i>
      </td>
      <td  style="background-color: #ecf0f1; border: 1px solid #7fb3d5; width: 120px;">
	N,NP<br>
	<i>$500M</i>
      </td>
      <td  style="background-color: #ecf0f1; border: 1px solid #7fb3d5; width: 120px;">
	P<br>
	<i>in</i>
      </td>
      <td  style="background-color: #ecf0f1; border: 1px solid #7fb3d5; width: 120px;">
	N,NP<br>
	<i>January</i>
      </td>
    </tr>
  </table>
</p>

<p>
  The notation VP<sup>3</sup> indicated that the symbol VP can be
  obtained in three different ways from the entries in the other
  cells. This also indicates that when building the parse trees, we
  obtain multiple different trees depending on which instance of VP we
  choose to expand. The three different ways to obtain the VP entry in
  (2,7) are: <i>(i)</i> from (2,5) (VP) and (6,7) (PP) using the rule
  VP &xrarr; VP PP; <i>(ii)</i> from (2,3) (VP) and (4,7) (PP) using
  the rule VP &xrarr; VP PP; <i>(iii)</i> from (2,2) (V) and (3,7) (NP)
  using the rule VP &xrarr; V NP.
</p>

<p>
  Likewise, the symbol VP in cell (2,5) and the symbol NP in cell
  (3,7) can be obtained in two different ways.
</p>

<p>
  The number of distinct parse trees that can be formed based on the
  CYK table is five, see below. The most sensible of them is the
  bottom one: in it, the subject is Google, and what they did is to
  buy something (DeepMind) for some amount of money ($500M) at some
  time (in January), i.e., both "for $500M" and "in January" are
  additional specifiers that are attached to the buying.  On the other
  hand, the first (top-left) parse tree, for instance, would be
  interpreted so that the object of buying is "DeepMind for $500M in
  January", which doesn't really constitute a meaningful expression
  (think for example what "This is DeepMind for $500M in January."
  would mean). Furthermore, the first parse tree includes the subtree
  "$500M in January", which is also an odd expression since $500M
  in January should be pretty much the same as $500M in February
  (except perhaps if you consider inflation-adjustment).
</p>

<p>
  <img src="/img/exercises/ex5/deepmind_parsetrees.png" width=100%><br><br>
</p>
<% end %>

<!-- Exercise 5.6 -->

<% partial 'partials/solution', locals: { name: 'Example solution' } do %>

<p>
  An example solution is <a href="https://materiaalit.github.io/intro-to-ai-18/files/part5-NLP-solution.zip">here</a>.
</p>


<% end %>
